<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Allocators" href="allocators.html" /><link rel="prev" title="Basics" href="../basics.html" />

    <!-- Generated with Sphinx 7.2.6 and Furo 2024.01.29 -->
        <title>Devices - Warp 1.0.2</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=135e06be" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=36a5483c" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=a91381f3" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  --admonition-title-font-size: 100%;
  --admonition-font-size: 100%;
  --color-api-pre-name: #4e9a06;
  --color-api-name: #4e9a06;
  --color-admonition-title--seealso: #ffffff;
  --color-admonition-title-background--seealso: #448aff;
  --color-admonition-title-background--note: #76b900;
  --color-admonition-title--note: #ffffff;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --color-admonition-title-background--note: #535353;
  --color-admonition-title--note: #ffffff;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --color-admonition-title-background--note: #535353;
  --color-admonition-title--note: #ffffff;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">Warp 1.0.2</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo only-light" src="../_static/logo-light-mode.png" alt="Light Logo"/>
    <img class="sidebar-logo only-dark" src="../_static/logo-dark-mode.png" alt="Dark Logo"/>
  </div>
  
  <span class="sidebar-brand-text">Warp 1.0.2</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">User's Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basics.html">Basics</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Devices</a></li>
<li class="toctree-l1"><a class="reference internal" href="allocators.html">Allocators</a></li>
<li class="toctree-l1"><a class="reference internal" href="concurrency.html">Concurrency</a></li>
<li class="toctree-l1"><a class="reference internal" href="generics.html">Generics</a></li>
<li class="toctree-l1"><a class="reference internal" href="interoperability.html">Interoperability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../configuration.html">Runtime Settings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debugging.html">Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../limitations.html">Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Core Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Runtime Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="functions.html">Kernel Reference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Simulation Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="sim.html">warp.sim</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">warp.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="fem.html">warp.fem</a></li>
<li class="toctree-l1"><a class="reference internal" href="render.html">warp.render</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Project Links</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/NVIDIA/warp">GitHub</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pypi.org/project/warp-lang">PyPI</a></li>
<li class="toctree-l1"><a class="reference external" href="https://discord.com/channels/827959428476174346/953756751977648148">Discord</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="devices">
<h1>Devices<a class="headerlink" href="#devices" title="Link to this heading">#</a></h1>
<p>Warp assigns unique string aliases to all supported compute devices in the system.  There is currently a single CPU device exposed as <code class="docutils literal notranslate"><span class="pre">&quot;cpu&quot;</span></code>.  Each CUDA-capable GPU gets an alias of the form <code class="docutils literal notranslate"><span class="pre">&quot;cuda:i&quot;</span></code>, where <code class="docutils literal notranslate"><span class="pre">i</span></code> is the CUDA device ordinal.  This convention should be familiar to users of other popular frameworks like PyTorch.</p>
<p>It is possible to explicitly target a specific device with each Warp API call using the <code class="docutils literal notranslate"><span class="pre">device</span></code> argument:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">a</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">a</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="n">b</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>
<span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">b</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">)</span>
<span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">c</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">c</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>A Warp CUDA device (<code class="docutils literal notranslate"><span class="pre">&quot;cuda:i&quot;</span></code>) corresponds to the primary CUDA context of device <code class="docutils literal notranslate"><span class="pre">i</span></code>.
This is compatible with frameworks like PyTorch and other software that uses the CUDA Runtime API.
It makes interoperability easy because GPU resources like memory can be shared with Warp.</p>
</div>
<dl class="py class">
<dt class="sig sig-object py" id="warp.context.Device">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">warp.context.</span></span><span class="sig-name descname"><span class="pre">Device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">runtime</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alias</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ordinal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_primary</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">context</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#warp.context.Device" title="Link to this definition">#</a></dt>
<dd><p>A device to allocate Warp arrays and to launch kernels on.</p>
<dl class="py attribute">
<dt class="sig sig-object py" id="warp.context.Device.ordinal">
<span class="sig-name descname"><span class="pre">ordinal</span></span><a class="headerlink" href="#warp.context.Device.ordinal" title="Link to this definition">#</a></dt>
<dd><p>A Warp-specific integer label for the device. <code class="docutils literal notranslate"><span class="pre">-1</span></code> for CPU devices.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="warp.context.Device.name">
<span class="sig-name descname"><span class="pre">name</span></span><a class="headerlink" href="#warp.context.Device.name" title="Link to this definition">#</a></dt>
<dd><p>A string label for the device. By default, CPU devices will be named according to the processor name,
or <code class="docutils literal notranslate"><span class="pre">&quot;CPU&quot;</span></code> if the processor name cannot be determined.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="warp.context.Device.arch">
<span class="sig-name descname"><span class="pre">arch</span></span><a class="headerlink" href="#warp.context.Device.arch" title="Link to this definition">#</a></dt>
<dd><p>An integer representing the compute capability version number calculated as
<code class="docutils literal notranslate"><span class="pre">10</span> <span class="pre">*</span> <span class="pre">major</span> <span class="pre">+</span> <span class="pre">minor</span></code>. <code class="docutils literal notranslate"><span class="pre">0</span></code> for CPU devices.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="warp.context.Device.is_uva">
<span class="sig-name descname"><span class="pre">is_uva</span></span><a class="headerlink" href="#warp.context.Device.is_uva" title="Link to this definition">#</a></dt>
<dd><p>A boolean indicating whether or not the device supports unified addressing.
<code class="docutils literal notranslate"><span class="pre">False</span></code> for CPU devices.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="warp.context.Device.is_cubin_supported">
<span class="sig-name descname"><span class="pre">is_cubin_supported</span></span><a class="headerlink" href="#warp.context.Device.is_cubin_supported" title="Link to this definition">#</a></dt>
<dd><p>A boolean indicating whether or not Warp’s version of NVRTC can directly
generate CUDA binary files (cubin) for this device’s architecture. <code class="docutils literal notranslate"><span class="pre">False</span></code> for CPU devices.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="warp.context.Device.is_mempool_supported">
<span class="sig-name descname"><span class="pre">is_mempool_supported</span></span><a class="headerlink" href="#warp.context.Device.is_mempool_supported" title="Link to this definition">#</a></dt>
<dd><p>A boolean indicating whether or not the device supports using the
<code class="docutils literal notranslate"><span class="pre">cuMemAllocAsync</span></code> and <code class="docutils literal notranslate"><span class="pre">cuMemPool</span></code> family of APIs for stream-ordered memory allocations. <code class="docutils literal notranslate"><span class="pre">False</span></code> for
CPU devices.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="warp.context.Device.is_primary">
<span class="sig-name descname"><span class="pre">is_primary</span></span><a class="headerlink" href="#warp.context.Device.is_primary" title="Link to this definition">#</a></dt>
<dd><p>A boolean indicating whether or not this device’s CUDA context is also the
device’s primary context.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="warp.context.Device.uuid">
<span class="sig-name descname"><span class="pre">uuid</span></span><a class="headerlink" href="#warp.context.Device.uuid" title="Link to this definition">#</a></dt>
<dd><p>A string representing the UUID of the CUDA device. The UUID is in the same format used by
<code class="docutils literal notranslate"><span class="pre">nvidia-smi</span> <span class="pre">-L</span></code>. <code class="docutils literal notranslate"><span class="pre">None</span></code> for CPU devices.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="warp.context.Device.pci_bus_id">
<span class="sig-name descname"><span class="pre">pci_bus_id</span></span><a class="headerlink" href="#warp.context.Device.pci_bus_id" title="Link to this definition">#</a></dt>
<dd><p>A string identifier for the CUDA device in the format <code class="docutils literal notranslate"><span class="pre">[domain]:[bus]:[device]</span></code>, in which
<code class="docutils literal notranslate"><span class="pre">domain</span></code>, <code class="docutils literal notranslate"><span class="pre">bus</span></code>, and <code class="docutils literal notranslate"><span class="pre">device</span></code> are all hexadecimal values. <code class="docutils literal notranslate"><span class="pre">None</span></code> for CPU devices.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="warp.context.Device.is_cpu">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">is_cpu</span></span><a class="headerlink" href="#warp.context.Device.is_cpu" title="Link to this definition">#</a></dt>
<dd><p>A boolean indicating whether or not the device is a CPU device.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="warp.context.Device.is_cuda">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">is_cuda</span></span><a class="headerlink" href="#warp.context.Device.is_cuda" title="Link to this definition">#</a></dt>
<dd><p>A boolean indicating whether or not the device is a CUDA device.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="warp.context.Device.context">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">context</span></span><a class="headerlink" href="#warp.context.Device.context" title="Link to this definition">#</a></dt>
<dd><p>The context associated with the device.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="warp.context.Device.has_context">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">has_context</span></span><a class="headerlink" href="#warp.context.Device.has_context" title="Link to this definition">#</a></dt>
<dd><p>A boolean indicating whether or not the device has a CUDA context associated with it.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="warp.context.Device.stream">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">stream</span></span><a class="headerlink" href="#warp.context.Device.stream" title="Link to this definition">#</a></dt>
<dd><p>The stream associated with a CUDA device.</p>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.12)"><strong>RuntimeError</strong></a> – The device is not a CUDA device.</p>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="warp.context.Device.has_stream">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">has_stream</span></span><a class="headerlink" href="#warp.context.Device.has_stream" title="Link to this definition">#</a></dt>
<dd><p>A boolean indicating whether or not the device has a stream associated with it.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="warp.context.Device.total_memory">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">total_memory</span></span><a class="headerlink" href="#warp.context.Device.total_memory" title="Link to this definition">#</a></dt>
<dd><p>The total amount of device memory available in bytes.</p>
<p>This function is currently only implemented for CUDA devices. 0 will be returned if called on a CPU device.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="warp.context.Device.free_memory">
<em class="property"><span class="pre">property</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">free_memory</span></span><a class="headerlink" href="#warp.context.Device.free_memory" title="Link to this definition">#</a></dt>
<dd><p>The amount of memory on the device that is free according to the OS in bytes.</p>
<p>This function is currently only implemented for CUDA devices. 0 will be returned if called on a CPU device.</p>
</dd></dl>

</dd></dl>

<section id="default-device">
<h2>Default Device<a class="headerlink" href="#default-device" title="Link to this heading">#</a></h2>
<p>To simplify writing code, Warp has the concept of <strong>default device</strong>.  When the <code class="docutils literal notranslate"><span class="pre">device</span></code> argument is omitted from a Warp API call, the default device will be used.</p>
<p>During Warp initialization, the default device is set to be <code class="docutils literal notranslate"><span class="pre">&quot;cuda:0&quot;</span></code> if CUDA is available.  Otherwise, the default device is <code class="docutils literal notranslate"><span class="pre">&quot;cpu&quot;</span></code>.</p>
<p>The function <code class="docutils literal notranslate"><span class="pre">wp.set_device()</span></code> can be used to change the default device:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">wp</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>

<span class="n">wp</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">b</span><span class="p">])</span>

<span class="n">wp</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
<span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">c</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">c</span><span class="p">])</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For CUDA devices, <code class="docutils literal notranslate"><span class="pre">wp.set_device()</span></code> does two things: it sets the Warp default device and it makes the device’s CUDA context current.  This helps to minimize the number of CUDA context switches in blocks of code targeting a single device.</p>
</div>
<p>For PyTorch users, this function is similar to <code class="docutils literal notranslate"><span class="pre">torch.cuda.set_device()</span></code>.  It is still possible to specify a different device in individual API calls, like in this snippet:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># set default device</span>
<span class="n">wp</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>

<span class="c1"># use default device</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

<span class="c1"># use explicit devices</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">)</span>

<span class="c1"># use default device</span>
<span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>

<span class="n">wp</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
<span class="n">wp</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="scoped-devices">
<h2>Scoped Devices<a class="headerlink" href="#scoped-devices" title="Link to this heading">#</a></h2>
<p>Another way to manage the default device is using <code class="docutils literal notranslate"><span class="pre">wp.ScopedDevice</span></code> objects.  They can be arbitrarily nested and restore the previous default device on exit:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">wp</span><span class="o">.</span><span class="n">ScopedDevice</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
    <span class="c1"># alloc and launch on &quot;cpu&quot;</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>

<span class="k">with</span> <span class="n">wp</span><span class="o">.</span><span class="n">ScopedDevice</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">):</span>
    <span class="c1"># alloc on &quot;cuda:0&quot;</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">wp</span><span class="o">.</span><span class="n">ScopedDevice</span><span class="p">(</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">):</span>
        <span class="c1"># alloc and launch on &quot;cuda:1&quot;</span>
        <span class="n">c</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
        <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">c</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">c</span><span class="p">])</span>

    <span class="c1"># launch on &quot;cuda:0&quot;</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">b</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">b</span><span class="p">])</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For CUDA devices, <code class="docutils literal notranslate"><span class="pre">wp.ScopedDevice</span></code> makes the device’s CUDA context current and restores the previous CUDA context on exit.  This is handy when running Warp scripts as part of a bigger pipeline, because it avoids any side effects of changing the CUDA context in the enclosed code.</p>
</div>
<section id="example-using-wp-scopeddevice-with-multiple-gpus">
<h3>Example: Using <code class="docutils literal notranslate"><span class="pre">wp.ScopedDevice</span></code> with multiple GPUs<a class="headerlink" href="#example-using-wp-scopeddevice-with-multiple-gpus" title="Link to this heading">#</a></h3>
<p>The following example shows how to allocate arrays and launch kernels on all available CUDA devices.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warp</span> <span class="k">as</span> <span class="nn">wp</span>

<span class="n">wp</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>


<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span> <span class="nf">inc</span><span class="p">(</span><span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+</span> <span class="mf">1.0</span>


<span class="c1"># get all CUDA devices</span>
<span class="n">devices</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">get_cuda_devices</span><span class="p">()</span>
<span class="n">device_count</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span>

<span class="c1"># number of launches</span>
<span class="n">iters</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># list of arrays, one per device</span>
<span class="n">arrs</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># loop over all devices</span>
<span class="k">for</span> <span class="n">device</span> <span class="ow">in</span> <span class="n">devices</span><span class="p">:</span>
    <span class="c1"># use a ScopedDevice to set the target device</span>
    <span class="k">with</span> <span class="n">wp</span><span class="o">.</span><span class="n">ScopedDevice</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
        <span class="c1"># allocate array</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">250</span> <span class="o">*</span> <span class="mi">1024</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
        <span class="n">arrs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

        <span class="c1"># launch kernels</span>
        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iters</span><span class="p">):</span>
            <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">inc</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>

<span class="c1"># synchronize all devices</span>
<span class="n">wp</span><span class="o">.</span><span class="n">synchronize</span><span class="p">()</span>

<span class="c1"># print results</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">device_count</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">arrs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2"> -&gt; </span><span class="si">{</span><span class="n">arrs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="current-cuda-device">
<h2>Current CUDA Device<a class="headerlink" href="#current-cuda-device" title="Link to this heading">#</a></h2>
<p>Warp uses the device alias <code class="docutils literal notranslate"><span class="pre">&quot;cuda&quot;</span></code> to target the current CUDA device.  This allows external code to manage the CUDA device on which to execute Warp scripts.  It is analogous to the PyTorch <code class="docutils literal notranslate"><span class="pre">&quot;cuda&quot;</span></code> device, which should be familiar to Torch users and simplify interoperation.</p>
<p>In this snippet, we use PyTorch to manage the current CUDA device and invoke a Warp kernel on that device:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">example_function</span><span class="p">():</span>
    <span class="c1"># create a Torch tensor on the current CUDA device</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

    <span class="n">a</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

    <span class="c1"># launch a Warp kernel on the current CUDA device</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">a</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

<span class="c1"># use Torch to set the current CUDA device and run example_function() on that device</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">example_function</span><span class="p">()</span>

<span class="c1"># use Torch to change the current CUDA device and re-run example_function() on that device</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">example_function</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Using the device alias <code class="docutils literal notranslate"><span class="pre">&quot;cuda&quot;</span></code> can be problematic if the code runs in an environment where another part of the code can unpredictably change the CUDA context.  Using an explicit CUDA device like <code class="docutils literal notranslate"><span class="pre">&quot;cuda:i&quot;</span></code> is recommended to avoid such issues.</p>
</div>
</section>
<section id="device-synchronization">
<h2>Device Synchronization<a class="headerlink" href="#device-synchronization" title="Link to this heading">#</a></h2>
<p>CUDA kernel launches and memory operations can execute asynchronously.  This allows for overlapping compute and memory operations on different devices.  Warp allows synchronizing the host with outstanding asynchronous operations on a specific device:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">wp</span><span class="o">.</span><span class="n">synchronize_device</span><span class="p">(</span><span class="s2">&quot;cuda:1&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">wp.synchronize_device()</span></code> function offers more fine-grained synchronization than <code class="docutils literal notranslate"><span class="pre">wp.synchronize()</span></code>, as the latter waits for <em>all</em> devices to complete their work.</p>
</section>
<section id="custom-cuda-contexts">
<h2>Custom CUDA Contexts<a class="headerlink" href="#custom-cuda-contexts" title="Link to this heading">#</a></h2>
<p>Warp is designed to work with arbitrary CUDA contexts so it can easily integrate into different workflows.</p>
<p>Applications built on the CUDA Runtime API target the <em>primary context</em> of each device.  The Runtime API hides CUDA context management under the hood.  In Warp, device <code class="docutils literal notranslate"><span class="pre">&quot;cuda:i&quot;</span></code> represents the primary context of device <code class="docutils literal notranslate"><span class="pre">i</span></code>, which aligns with the CUDA Runtime API.</p>
<p>Applications built on the CUDA Driver API work with CUDA contexts directly and can create custom CUDA contexts on any device.  Custom CUDA contexts can be created with specific affinity or interop features that benefit the application.  Warp can work with these CUDA contexts as well.</p>
<p>The special device alias <code class="docutils literal notranslate"><span class="pre">&quot;cuda&quot;</span></code> can be used to target the current CUDA context, whether this is a primary or custom context.</p>
<p>In addition, Warp allows registering new device aliases for custom CUDA contexts, so that they can be explicitly targeted by name.  If the <code class="docutils literal notranslate"><span class="pre">CUcontext</span></code> pointer is available, it can be used to create a new device alias like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">wp</span><span class="o">.</span><span class="n">map_cuda_device</span><span class="p">(</span><span class="s2">&quot;foo&quot;</span><span class="p">,</span> <span class="n">ctypes</span><span class="o">.</span><span class="n">c_void_p</span><span class="p">(</span><span class="n">context_ptr</span><span class="p">))</span>
</pre></div>
</div>
<p>Alternatively, if the custom CUDA context was made current by the application, the pointer can be omitted:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">wp</span><span class="o">.</span><span class="n">map_cuda_device</span><span class="p">(</span><span class="s2">&quot;foo&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>In either case, mapping the custom CUDA context allows us to target the context directly using the assigned alias:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">wp</span><span class="o">.</span><span class="n">ScopedDevice</span><span class="p">(</span><span class="s2">&quot;foo&quot;</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="cuda-peer-access">
<span id="peer-access"></span><h2>CUDA Peer Access<a class="headerlink" href="#cuda-peer-access" title="Link to this heading">#</a></h2>
<p>CUDA allows direct memory access between different GPUs if the system hardware configuration supports it.  Typically, the GPUs should be of the same type and a special interconnect may be required (e.g., NVLINK or PCIe topology).</p>
<p>During initialization, Warp reports whether peer access is supported on multi-GPU systems:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Warp 0.15.1 initialized:
   CUDA Toolkit 11.5, Driver 12.2
   Devices:
     &quot;cpu&quot;      : &quot;x86_64&quot;
     &quot;cuda:0&quot;   : &quot;NVIDIA L40&quot; (48 GiB, sm_89, mempool enabled)
     &quot;cuda:1&quot;   : &quot;NVIDIA L40&quot; (48 GiB, sm_89, mempool enabled)
     &quot;cuda:2&quot;   : &quot;NVIDIA L40&quot; (48 GiB, sm_89, mempool enabled)
     &quot;cuda:3&quot;   : &quot;NVIDIA L40&quot; (48 GiB, sm_89, mempool enabled)
   CUDA peer access:
     Supported fully (all-directional)
</pre></div>
</div>
<p>If the message reports that CUDA peer access is <code class="docutils literal notranslate"><span class="pre">Supported</span> <span class="pre">fully</span></code>, it means that every CUDA device can access every other CUDA device in the system.  If it says <code class="docutils literal notranslate"><span class="pre">Supported</span> <span class="pre">partially</span></code>, it will be followed by the access matrix that shows which devices can access each other.  If it says <code class="docutils literal notranslate"><span class="pre">Not</span> <span class="pre">supported</span></code>, it means that access is not supported between any devices.</p>
<p>In code, we can check support and enable peer access like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">wp</span><span class="o">.</span><span class="n">is_peer_access_supported</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">,</span> <span class="s2">&quot;cuda:1&quot;</span><span class="p">):</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">set_peer_access_enabled</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">,</span> <span class="s2">&quot;cuda:1&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">):</span>
</pre></div>
</div>
<p>This will allow the memory of device <code class="docutils literal notranslate"><span class="pre">cuda:0</span></code> to be directly accessed on device <code class="docutils literal notranslate"><span class="pre">cuda:1</span></code>.  Peer access is directional, which means that enabling access to <code class="docutils literal notranslate"><span class="pre">cuda:0</span></code> from <code class="docutils literal notranslate"><span class="pre">cuda:1</span></code> does not automatically enable access to <code class="docutils literal notranslate"><span class="pre">cuda:1</span></code> from <code class="docutils literal notranslate"><span class="pre">cuda:0</span></code>.</p>
<p>The benefit of enabling peer access is that it allows direct memory transfers (DMA) between the devices.  This is generally a faster way to copy data, since otherwise the transfer needs to be done using a CPU staging buffer.</p>
<p>The drawback is that enabling peer access can reduce the performance of allocations and deallocations.  Programs that don’t rely on peer-to-peer memory transfers should leave this setting disabled.</p>
<p>It’s possible to temporarily enable or disable peer access using a scoped manager:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">wp</span><span class="o">.</span><span class="n">ScopedPeerAccess</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">,</span> <span class="s2">&quot;cuda:1&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Peer access does not accelerate memory transfers between arrays allocated using the <a class="reference internal" href="allocators.html#mempool-allocators"><span class="std std-ref">stream-ordered memory pool allocators</span></a> introduced in Warp 0.14.0.  To accelerate memory pool transfers, <a class="reference internal" href="allocators.html#mempool-access"><span class="std std-ref">memory pool access</span></a> should be enabled instead.</p>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="warp.is_peer_access_supported">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">is_peer_access_supported</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target_device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">peer_device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#warp.is_peer_access_supported" title="Link to this definition">#</a></dt>
<dd><p>Check if <cite>peer_device</cite> can directly access the memory of <cite>target_device</cite> on this system.</p>
<p>This applies to memory allocated using default CUDA allocators.  For memory allocated using
CUDA pooled allocators, use <cite>is_mempool_access_supported()</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A Boolean value indicating if this peer access is supported by the system.</p>
</dd>
<dt class="field-even">Parameters<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>target_device</strong> (<a class="reference internal" href="#warp.context.Device" title="warp.context.Device"><em>Device</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em> | </em><em>None</em>) – </p></li>
<li><p><strong>peer_device</strong> (<a class="reference internal" href="#warp.context.Device" title="warp.context.Device"><em>Device</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em> | </em><em>None</em>) – </p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.is_peer_access_enabled">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">is_peer_access_enabled</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target_device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">peer_device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#warp.is_peer_access_enabled" title="Link to this definition">#</a></dt>
<dd><p>Check if <cite>peer_device</cite> can currently access the memory of <cite>target_device</cite>.</p>
<p>This applies to memory allocated using default CUDA allocators.  For memory allocated using
CUDA pooled allocators, use <cite>is_mempool_access_enabled()</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A Boolean value indicating if this peer access is currently enabled.</p>
</dd>
<dt class="field-even">Parameters<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>target_device</strong> (<a class="reference internal" href="#warp.context.Device" title="warp.context.Device"><em>Device</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em> | </em><em>None</em>) – </p></li>
<li><p><strong>peer_device</strong> (<a class="reference internal" href="#warp.context.Device" title="warp.context.Device"><em>Device</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em> | </em><em>None</em>) – </p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.set_peer_access_enabled">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">set_peer_access_enabled</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target_device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">peer_device</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#warp.set_peer_access_enabled" title="Link to this definition">#</a></dt>
<dd><p>Enable or disable direct access from <cite>peer_device</cite> to the memory of <cite>target_device</cite>.</p>
<p>Enabling peer access can improve the speed of peer-to-peer memory transfers, but can have
a negative impact on memory consumption and allocation performance.</p>
<p>This applies to memory allocated using default CUDA allocators.  For memory allocated using
CUDA pooled allocators, use <cite>set_mempool_access_enabled()</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>target_device</strong> (<a class="reference internal" href="#warp.context.Device" title="warp.context.Device"><em>Device</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em> | </em><em>None</em>) – </p></li>
<li><p><strong>peer_device</strong> (<a class="reference internal" href="#warp.context.Device" title="warp.context.Device"><em>Device</em></a><em> | </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)"><em>str</em></a><em> | </em><em>None</em>) – </p></li>
<li><p><strong>enable</strong> (<a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><em>bool</em></a>) – </p></li>
</ul>
</dd>
</dl>
</dd></dl>

</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="allocators.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Allocators</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="../basics.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Basics</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2022-2024, NVIDIA
            </div>
            Made with 
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link " href="https://github.com/NVIDIA/warp" aria-label="GitHub">
            <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16">
                <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path>
            </svg>
        </a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Devices</a><ul>
<li><a class="reference internal" href="#warp.context.Device"><code class="docutils literal notranslate"><span class="pre">Device</span></code></a><ul>
<li><a class="reference internal" href="#warp.context.Device.ordinal"><code class="docutils literal notranslate"><span class="pre">Device.ordinal</span></code></a></li>
<li><a class="reference internal" href="#warp.context.Device.name"><code class="docutils literal notranslate"><span class="pre">Device.name</span></code></a></li>
<li><a class="reference internal" href="#warp.context.Device.arch"><code class="docutils literal notranslate"><span class="pre">Device.arch</span></code></a></li>
<li><a class="reference internal" href="#warp.context.Device.is_uva"><code class="docutils literal notranslate"><span class="pre">Device.is_uva</span></code></a></li>
<li><a class="reference internal" href="#warp.context.Device.is_cubin_supported"><code class="docutils literal notranslate"><span class="pre">Device.is_cubin_supported</span></code></a></li>
<li><a class="reference internal" href="#warp.context.Device.is_mempool_supported"><code class="docutils literal notranslate"><span class="pre">Device.is_mempool_supported</span></code></a></li>
<li><a class="reference internal" href="#warp.context.Device.is_primary"><code class="docutils literal notranslate"><span class="pre">Device.is_primary</span></code></a></li>
<li><a class="reference internal" href="#warp.context.Device.uuid"><code class="docutils literal notranslate"><span class="pre">Device.uuid</span></code></a></li>
<li><a class="reference internal" href="#warp.context.Device.pci_bus_id"><code class="docutils literal notranslate"><span class="pre">Device.pci_bus_id</span></code></a></li>
<li><a class="reference internal" href="#warp.context.Device.is_cpu"><code class="docutils literal notranslate"><span class="pre">Device.is_cpu</span></code></a></li>
<li><a class="reference internal" href="#warp.context.Device.is_cuda"><code class="docutils literal notranslate"><span class="pre">Device.is_cuda</span></code></a></li>
<li><a class="reference internal" href="#warp.context.Device.context"><code class="docutils literal notranslate"><span class="pre">Device.context</span></code></a></li>
<li><a class="reference internal" href="#warp.context.Device.has_context"><code class="docutils literal notranslate"><span class="pre">Device.has_context</span></code></a></li>
<li><a class="reference internal" href="#warp.context.Device.stream"><code class="docutils literal notranslate"><span class="pre">Device.stream</span></code></a></li>
<li><a class="reference internal" href="#warp.context.Device.has_stream"><code class="docutils literal notranslate"><span class="pre">Device.has_stream</span></code></a></li>
<li><a class="reference internal" href="#warp.context.Device.total_memory"><code class="docutils literal notranslate"><span class="pre">Device.total_memory</span></code></a></li>
<li><a class="reference internal" href="#warp.context.Device.free_memory"><code class="docutils literal notranslate"><span class="pre">Device.free_memory</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#default-device">Default Device</a></li>
<li><a class="reference internal" href="#scoped-devices">Scoped Devices</a><ul>
<li><a class="reference internal" href="#example-using-wp-scopeddevice-with-multiple-gpus">Example: Using <code class="docutils literal notranslate"><span class="pre">wp.ScopedDevice</span></code> with multiple GPUs</a></li>
</ul>
</li>
<li><a class="reference internal" href="#current-cuda-device">Current CUDA Device</a></li>
<li><a class="reference internal" href="#device-synchronization">Device Synchronization</a></li>
<li><a class="reference internal" href="#custom-cuda-contexts">Custom CUDA Contexts</a></li>
<li><a class="reference internal" href="#cuda-peer-access">CUDA Peer Access</a><ul>
<li><a class="reference internal" href="#warp.is_peer_access_supported"><code class="docutils literal notranslate"><span class="pre">is_peer_access_supported()</span></code></a></li>
<li><a class="reference internal" href="#warp.is_peer_access_enabled"><code class="docutils literal notranslate"><span class="pre">is_peer_access_enabled()</span></code></a></li>
<li><a class="reference internal" href="#warp.set_peer_access_enabled"><code class="docutils literal notranslate"><span class="pre">set_peer_access_enabled()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=1ed6394b"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=32e29ea5"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=fd10adb8"></script>
    </body>
</html>