<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" /><link rel="next" title="Runtime Settings" href="../configuration.html" /><link rel="prev" title="Generics" href="generics.html" />

    <!-- Generated with Sphinx 7.2.6 and Furo 2024.01.29 -->
        <title>Interoperability - Warp 1.0.2</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=135e06be" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=36a5483c" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=a91381f3" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  --admonition-title-font-size: 100%;
  --admonition-font-size: 100%;
  --color-api-pre-name: #4e9a06;
  --color-api-name: #4e9a06;
  --color-admonition-title--seealso: #ffffff;
  --color-admonition-title-background--seealso: #448aff;
  --color-admonition-title-background--note: #76b900;
  --color-admonition-title--note: #ffffff;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --color-admonition-title-background--note: #535353;
  --color-admonition-title--note: #ffffff;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --color-admonition-title-background--note: #535353;
  --color-admonition-title--note: #ffffff;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-half" viewBox="0 0 24 24">
    <title>Auto light/dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1.5" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-shadow">
      <path stroke="none" d="M0 0h24v24H0z" fill="none"/>
      <circle cx="12" cy="12" r="9" />
      <path d="M13 12h5" />
      <path d="M13 15h4" />
      <path d="M13 18h1" />
      <path d="M13 9h4" />
      <path d="M13 6h1" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">Warp 1.0.2</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo only-light" src="../_static/logo-light-mode.png" alt="Light Logo"/>
    <img class="sidebar-logo only-dark" src="../_static/logo-dark-mode.png" alt="Dark Logo"/>
  </div>
  
  <span class="sidebar-brand-text">Warp 1.0.2</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <p class="caption" role="heading"><span class="caption-text">User's Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basics.html">Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="devices.html">Devices</a></li>
<li class="toctree-l1"><a class="reference internal" href="allocators.html">Allocators</a></li>
<li class="toctree-l1"><a class="reference internal" href="concurrency.html">Concurrency</a></li>
<li class="toctree-l1"><a class="reference internal" href="generics.html">Generics</a></li>
<li class="toctree-l1 current current-page"><a class="current reference internal" href="#">Interoperability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../configuration.html">Runtime Settings</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debugging.html">Debugging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../limitations.html">Limitations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">FAQ</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Core Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="runtime.html">Runtime Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="functions.html">Kernel Reference</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Simulation Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="sim.html">warp.sim</a></li>
<li class="toctree-l1"><a class="reference internal" href="sparse.html">warp.sparse</a></li>
<li class="toctree-l1"><a class="reference internal" href="fem.html">warp.fem</a></li>
<li class="toctree-l1"><a class="reference internal" href="render.html">warp.render</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Project Links</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://github.com/NVIDIA/warp">GitHub</a></li>
<li class="toctree-l1"><a class="reference external" href="https://pypi.org/project/warp-lang">PyPI</a></li>
<li class="toctree-l1"><a class="reference external" href="https://discord.com/channels/827959428476174346/953756751977648148">Discord</a></li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto"><use href="#svg-sun-half"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main">
          <section id="interoperability">
<h1>Interoperability<a class="headerlink" href="#interoperability" title="Link to this heading">#</a></h1>
<p>Warp can interop with other Python-based frameworks such as NumPy through standard interface protocols.</p>
<section id="numpy">
<h2>NumPy<a class="headerlink" href="#numpy" title="Link to this heading">#</a></h2>
<p>Warp arrays may be converted to a NumPy array through the <code class="docutils literal notranslate"><span class="pre">warp.array.numpy()</span></code> method. When the Warp array lives on
the <code class="docutils literal notranslate"><span class="pre">cpu</span></code> device this will return a zero-copy view onto the underlying Warp allocation. If the array lives on a
<code class="docutils literal notranslate"><span class="pre">cuda</span></code> device then it will first be copied back to a temporary buffer and copied to NumPy.</p>
<p>Warp CPU arrays also implement  the <code class="docutils literal notranslate"><span class="pre">__array_interface__</span></code> protocol and so can be used to construct NumPy arrays
directly:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
<span class="o">&gt;</span> <span class="p">[</span><span class="mf">1.</span> <span class="mf">2.</span> <span class="mf">3.</span><span class="p">]</span>
</pre></div>
</div>
<p>Data type conversion utilities are also available for convenience:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">warp_type</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">float32</span>
<span class="o">...</span>
<span class="n">numpy_type</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">dtype_to_numpy</span><span class="p">(</span><span class="n">warp_type</span><span class="p">)</span>
<span class="o">...</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">warp_type</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">numpy_type</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="warp.dtype_from_numpy">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">dtype_from_numpy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">numpy_dtype</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#warp.dtype_from_numpy" title="Link to this definition">#</a></dt>
<dd><p>Return the Warp dtype corresponding to a NumPy dtype.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.dtype_to_numpy">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">dtype_to_numpy</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">warp_dtype</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#warp.dtype_to_numpy" title="Link to this definition">#</a></dt>
<dd><p>Return the NumPy dtype corresponding to a Warp dtype.</p>
</dd></dl>

</section>
<section id="pytorch">
<span id="pytorch-interop"></span><h2>PyTorch<a class="headerlink" href="#pytorch" title="Link to this heading">#</a></h2>
<p>Warp provides helper functions to convert arrays to/from PyTorch:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

<span class="c1"># convert to Torch tensor</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">to_torch</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

<span class="c1"># convert from Torch tensor</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
</pre></div>
</div>
<p>These helper functions allow the conversion of Warp arrays to/from PyTorch tensors without copying the underlying data.
At the same time, if available, gradient arrays and tensors are converted to/from PyTorch autograd tensors, allowing the use of Warp arrays
in PyTorch autograd computations.</p>
<section id="example-optimization-using-warp-from-torch">
<h3>Example: Optimization using <code class="docutils literal notranslate"><span class="pre">warp.from_torch()</span></code><a class="headerlink" href="#example-optimization-using-warp-from-torch" title="Link to this heading">#</a></h3>
<p>An example usage of minimizing a loss function over an array of 2D points written in Warp via PyTorch’s Adam optimizer using <code class="docutils literal notranslate"><span class="pre">warp.from_torch</span></code> is as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warp</span> <span class="k">as</span> <span class="nn">wp</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">wp</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">xs</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">l</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">atomic_add</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">)</span>

<span class="c1"># indicate requires_grad so that Warp can accumulate gradients in the grad buffers</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">xs</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">wp_xs</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">xs</span><span class="p">)</span>
<span class="n">wp_l</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>

<span class="n">tape</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">Tape</span><span class="p">()</span>
<span class="k">with</span> <span class="n">tape</span><span class="p">:</span>
    <span class="c1"># record the loss function kernel launch on the tape</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">wp_xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">wp_l</span><span class="p">)</span>  <span class="c1"># compute gradients</span>
    <span class="c1"># now xs.grad will be populated with the gradients computed by Warp</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>  <span class="c1"># update xs (and thereby wp_xs)</span>

    <span class="c1"># these lines are only needed for evaluating the loss</span>
    <span class="c1"># (the optimization just needs the gradient, not the loss value)</span>
    <span class="n">wp_l</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">wp_l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">wp_xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="se">\t</span><span class="s2">loss: </span><span class="si">{</span><span class="n">l</span><span class="o">.</span><span class="n">item</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="example-optimization-using-warp-to-torch">
<h3>Example: Optimization using <code class="docutils literal notranslate"><span class="pre">warp.to_torch</span></code><a class="headerlink" href="#example-optimization-using-warp-to-torch" title="Link to this heading">#</a></h3>
<p>Less code is needed when we declare the optimization variables directly in Warp and use <code class="docutils literal notranslate"><span class="pre">warp.to_torch</span></code> to convert them to PyTorch tensors.
Here, we revisit the same example from above where now only a single conversion to a torch tensor is needed to supply Adam with the optimization variables:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warp</span> <span class="k">as</span> <span class="nn">wp</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">wp</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">xs</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span> <span class="n">ndim</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="n">l</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">atomic_add</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="n">xs</span><span class="p">[</span><span class="n">tid</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mf">2.0</span><span class="p">)</span>

<span class="c1"># initialize the optimization variables in Warp</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="c1"># just a single wp.to_torch call is needed, Adam optimizes using the Warp array gradients</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">wp</span><span class="o">.</span><span class="n">to_torch</span><span class="p">(</span><span class="n">xs</span><span class="p">)],</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">tape</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">Tape</span><span class="p">()</span>
<span class="k">with</span> <span class="n">tape</span><span class="p">:</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">500</span><span class="p">):</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">zero</span><span class="p">()</span>
    <span class="n">tape</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="n">l</span><span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="n">l</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
    <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">xs</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">xs</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="se">\t</span><span class="s2">loss: </span><span class="si">{</span><span class="n">l</span><span class="o">.</span><span class="n">numpy</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="example-optimization-using-torch-autograd-function">
<h3>Example: Optimization using <code class="docutils literal notranslate"><span class="pre">torch.autograd.function</span></code><a class="headerlink" href="#example-optimization-using-torch-autograd-function" title="Link to this heading">#</a></h3>
<p>One can insert Warp kernel launches in a PyTorch graph by defining a <code class="docutils literal notranslate"><span class="pre">torch.autograd.function</span></code> class, which
requires forward and backward functions to be defined. After mapping incoming torch arrays to Warp arrays, a Warp kernel
may be launched in the usual way. In the backward pass, the same kernel’s adjoint may be launched by
setting <code class="docutils literal notranslate"><span class="pre">adjoint</span> <span class="pre">=</span> <span class="pre">True</span></code> in <code class="xref py py-func docutils literal notranslate"><span class="pre">wp.launch()</span></code>. Alternatively, the user may choose to rely on Warp’s tape.
In the following example, we demonstrate how Warp may be used to evaluate the Rosenbrock function in an optimization context:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warp</span> <span class="k">as</span> <span class="nn">wp</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">wp</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="n">pvec2</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">types</span><span class="o">.</span><span class="n">vector</span><span class="p">(</span><span class="n">length</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">wp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

<span class="c1"># Define the Rosenbrock function</span>
<span class="nd">@wp</span><span class="o">.</span><span class="n">func</span>
<span class="k">def</span> <span class="nf">rosenbrock</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span> <span class="o">+</span> <span class="mf">100.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">x</span><span class="o">**</span><span class="mf">2.0</span><span class="p">)</span> <span class="o">**</span> <span class="mf">2.0</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span> <span class="nf">eval_rosenbrock</span><span class="p">(</span>
    <span class="n">xs</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">pvec2</span><span class="p">),</span>
    <span class="c1"># outputs</span>
    <span class="n">z</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
<span class="p">):</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">xs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    <span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">rosenbrock</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>


<span class="k">class</span> <span class="nc">Rosenbrock</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">xy</span><span class="p">,</span> <span class="n">num_points</span><span class="p">):</span>
        <span class="c1"># ensure Torch operations complete before running Warp</span>
        <span class="n">wp</span><span class="o">.</span><span class="n">synchronize_device</span><span class="p">()</span>

        <span class="n">ctx</span><span class="o">.</span><span class="n">xy</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">pvec2</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">num_points</span> <span class="o">=</span> <span class="n">num_points</span>

        <span class="c1"># allocate output</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_points</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span>
            <span class="n">kernel</span><span class="o">=</span><span class="n">eval_rosenbrock</span><span class="p">,</span>
            <span class="n">dim</span><span class="o">=</span><span class="n">ctx</span><span class="o">.</span><span class="n">num_points</span><span class="p">,</span>
            <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">xy</span><span class="p">],</span>
            <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">z</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="c1"># ensure Warp operations complete before returning data to Torch</span>
        <span class="n">wp</span><span class="o">.</span><span class="n">synchronize_device</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">wp</span><span class="o">.</span><span class="n">to_torch</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">z</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">adj_z</span><span class="p">):</span>
        <span class="c1"># ensure Torch operations complete before running Warp</span>
        <span class="n">wp</span><span class="o">.</span><span class="n">synchronize_device</span><span class="p">()</span>

        <span class="c1"># map incoming Torch grads to our output variables</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_torch</span><span class="p">(</span><span class="n">adj_z</span><span class="p">)</span>

        <span class="n">wp</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span>
            <span class="n">kernel</span><span class="o">=</span><span class="n">eval_rosenbrock</span><span class="p">,</span>
            <span class="n">dim</span><span class="o">=</span><span class="n">ctx</span><span class="o">.</span><span class="n">num_points</span><span class="p">,</span>
            <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">xy</span><span class="p">],</span>
            <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">z</span><span class="p">],</span>
            <span class="n">adj_inputs</span><span class="o">=</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">xy</span><span class="o">.</span><span class="n">grad</span><span class="p">],</span>
            <span class="n">adj_outputs</span><span class="o">=</span><span class="p">[</span><span class="n">ctx</span><span class="o">.</span><span class="n">z</span><span class="o">.</span><span class="n">grad</span><span class="p">],</span>
            <span class="n">adjoint</span><span class="o">=</span><span class="kc">True</span>
        <span class="p">)</span>

        <span class="c1"># ensure Warp operations complete before returning data to Torch</span>
        <span class="n">wp</span><span class="o">.</span><span class="n">synchronize_device</span><span class="p">()</span>

        <span class="c1"># return adjoint w.r.t. inputs</span>
        <span class="k">return</span> <span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">to_torch</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">xy</span><span class="o">.</span><span class="n">grad</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>


<span class="n">num_points</span> <span class="o">=</span> <span class="mi">1500</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">5e-2</span>

<span class="n">torch_device</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">device_to_torch</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">get_device</span><span class="p">())</span>

<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">xy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">num_points</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">torch_device</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">([</span><span class="n">xy</span><span class="p">],</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
    <span class="c1"># step</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">Rosenbrock</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">xy</span><span class="p">,</span> <span class="n">num_points</span><span class="p">)</span>
    <span class="n">z</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>

    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

<span class="c1"># minimum at (1, 1)</span>
<span class="n">xy_np</span> <span class="o">=</span> <span class="n">xy</span><span class="o">.</span><span class="n">numpy</span><span class="p">(</span><span class="n">force</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">xy_np</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="warp.from_torch">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">from_torch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">t</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#warp.from_torch" title="Link to this definition">#</a></dt>
<dd><p>Convert a Torch tensor to a Warp array without copying the data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>t</strong> (<em>torch.Tensor</em>) – The torch tensor to wrap.</p></li>
<li><p><strong>dtype</strong> (<em>warp.dtype</em><em>, </em><em>optional</em>) – The target data type of the resulting Warp array. Defaults to the tensor value type mapped to a Warp array value type.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><em>bool</em></a><em>, </em><em>optional</em>) – Whether the resulting array should wrap the tensor’s gradient, if it exists (the grad tensor will be allocated otherwise). Defaults to the tensor’s <cite>requires_grad</cite> value.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The wrapped array.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="runtime.html#warp.array" title="warp.array">warp.array</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.to_torch">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">to_torch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">a</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#warp.to_torch" title="Link to this definition">#</a></dt>
<dd><p>Convert a Warp array to a Torch tensor without copying the data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a</strong> (<a class="reference internal" href="runtime.html#warp.array" title="warp.array"><em>warp.array</em></a>) – The Warp array to convert.</p></li>
<li><p><strong>requires_grad</strong> (<a class="reference internal" href="functions.html#warp.bool" title="warp.bool"><em>bool</em></a><em>, </em><em>optional</em>) – Whether the resulting tensor should convert the array’s gradient, if it exists, to a grad tensor. Defaults to the array’s <cite>requires_grad</cite> value.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The converted tensor.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.device_from_torch">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">device_from_torch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">torch_device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#warp.device_from_torch" title="Link to this definition">#</a></dt>
<dd><p>Return the Warp device corresponding to a Torch device.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.device_to_torch">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">device_to_torch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">warp_device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#warp.device_to_torch" title="Link to this definition">#</a></dt>
<dd><p>Return the Torch device corresponding to a Warp device.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.dtype_from_torch">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">dtype_from_torch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">torch_dtype</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#warp.dtype_from_torch" title="Link to this definition">#</a></dt>
<dd><p>Return the Warp dtype corresponding to a Torch dtype.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.dtype_to_torch">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">dtype_to_torch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">warp_dtype</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#warp.dtype_to_torch" title="Link to this definition">#</a></dt>
<dd><p>Return the Torch dtype corresponding to a Warp dtype.</p>
</dd></dl>

</section>
</section>
<section id="cupy-numba">
<h2>CuPy/Numba<a class="headerlink" href="#cupy-numba" title="Link to this heading">#</a></h2>
<p>Warp GPU arrays support the <code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code> protocol for sharing data with other Python GPU frameworks.
Currently this is one-directional, so that Warp arrays can be used as input to any framework that also supports the
<code class="docutils literal notranslate"><span class="pre">__cuda_array_interface__</span></code> protocol, but not the other way around.</p>
</section>
<section id="jax">
<span id="jax-interop"></span><h2>JAX<a class="headerlink" href="#jax" title="Link to this heading">#</a></h2>
<p>Interoperability with JAX arrays is supported through the following methods.
Internally these use the DLPack protocol to exchange data in a zero-copy way with JAX:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">warp_array</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_jax</span><span class="p">(</span><span class="n">jax_array</span><span class="p">)</span>
<span class="n">jax_array</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">to_jax</span><span class="p">(</span><span class="n">warp_array</span><span class="p">)</span>
</pre></div>
</div>
<p>It may be preferable to use the <a class="reference internal" href="#dlpack"><span class="std std-ref">DLPack</span></a> protocol directly for better performance and control over stream synchronization behaviour.</p>
<dl class="py function">
<dt class="sig sig-object py" id="warp.from_jax">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">from_jax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">jax_array</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#warp.from_jax" title="Link to this definition">#</a></dt>
<dd><p>Convert a Jax array to a Warp array without copying the data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>jax_array</strong> – The Jax array to convert.</p></li>
<li><p><strong>dtype</strong> (<em>optional</em>) – The target data type of the resulting Warp array. Defaults to the Jax array’s data type mapped to a Warp data type.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The converted Warp array.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="runtime.html#warp.array" title="warp.array">warp.array</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.to_jax">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">to_jax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">warp_array</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#warp.to_jax" title="Link to this definition">#</a></dt>
<dd><p>Convert a Warp array to a Jax array without copying the data.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>warp_array</strong> (<a class="reference internal" href="runtime.html#warp.array" title="warp.array"><em>warp.array</em></a>) – The Warp array to convert.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The converted Jax array.</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.device_from_jax">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">device_from_jax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">jax_device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#warp.device_from_jax" title="Link to this definition">#</a></dt>
<dd><p>Return the Warp device corresponding to a Jax device.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.device_to_jax">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">device_to_jax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">warp_device</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#warp.device_to_jax" title="Link to this definition">#</a></dt>
<dd><p>Return the Jax device corresponding to a Warp device.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.dtype_from_jax">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">dtype_from_jax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">jax_dtype</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#warp.dtype_from_jax" title="Link to this definition">#</a></dt>
<dd><p>Return the Warp dtype corresponding to a Jax dtype.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.dtype_to_jax">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">dtype_to_jax</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">warp_dtype</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#warp.dtype_to_jax" title="Link to this definition">#</a></dt>
<dd><p>Return the Jax dtype corresponding to a Warp dtype.</p>
</dd></dl>

<section id="using-warp-kernels-as-jax-primitives">
<h3>Using Warp kernels as JAX primitives<a class="headerlink" href="#using-warp-kernels-as-jax-primitives" title="Link to this heading">#</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This is an experimental feature under development.</p>
</div>
<p>Warp kernels can be used as JAX primitives, which can be used to call Warp kernels inside of jitted JAX functions:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warp</span> <span class="k">as</span> <span class="nn">wp</span>
<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jp</span>

<span class="c1"># import experimental feature</span>
<span class="kn">from</span> <span class="nn">warp.jax_experimental</span> <span class="kn">import</span> <span class="n">jax_kernel</span>

<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span> <span class="nf">triple_kernel</span><span class="p">(</span><span class="nb">input</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span> <span class="n">output</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">output</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="mf">3.0</span> <span class="o">*</span> <span class="nb">input</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span>

<span class="n">wp</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="c1"># create a Jax primitive from a Warp kernel</span>
<span class="n">jax_triple</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">triple_kernel</span><span class="p">)</span>

<span class="c1"># use the Warp kernel in a Jax jitted function</span>
<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">jp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jax_triple</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">())</span>
</pre></div>
</div>
<p>Since this is an experimental feature, there are some limitations:</p>
<blockquote>
<div><ul class="simple">
<li><p>All kernel arguments must be arrays.</p></li>
<li><p>Kernel launch dimensions are inferred from the shape of the first argument.</p></li>
<li><p>Input arguments are followed by output arguments in the Warp kernel definition.</p></li>
<li><p>There must be at least one input argument and at least one output argument.</p></li>
<li><p>Output shapes must match the launch dimensions (i.e., output shapes must match the shape of the first argument).</p></li>
<li><p>All arrays must be contiguous.</p></li>
<li><p>Only the CUDA backend is supported.</p></li>
</ul>
</div></blockquote>
<p>Here is an example of an operation with three inputs and two outputs:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warp</span> <span class="k">as</span> <span class="nn">wp</span>
<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jp</span>

<span class="c1"># import experimental feature</span>
<span class="kn">from</span> <span class="nn">warp.jax_experimental</span> <span class="kn">import</span> <span class="n">jax_kernel</span>

<span class="c1"># kernel with multiple inputs and outputs</span>
<span class="nd">@wp</span><span class="o">.</span><span class="n">kernel</span>
<span class="k">def</span> <span class="nf">multiarg_kernel</span><span class="p">(</span>
    <span class="c1"># inputs</span>
    <span class="n">a</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="n">b</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="n">c</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="c1"># outputs</span>
    <span class="n">ab</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
    <span class="n">bc</span><span class="p">:</span> <span class="n">wp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">),</span>
<span class="p">):</span>
    <span class="n">tid</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">tid</span><span class="p">()</span>
    <span class="n">ab</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+</span> <span class="n">b</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span>
    <span class="n">bc</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">=</span> <span class="n">b</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span> <span class="o">+</span> <span class="n">c</span><span class="p">[</span><span class="n">tid</span><span class="p">]</span>

<span class="n">wp</span><span class="o">.</span><span class="n">init</span><span class="p">()</span>

<span class="c1"># create a Jax primitive from a Warp kernel</span>
<span class="n">jax_multiarg</span> <span class="o">=</span> <span class="n">jax_kernel</span><span class="p">(</span><span class="n">multiarg_kernel</span><span class="p">)</span>

<span class="c1"># use the Warp kernel in a Jax jitted function with three inputs and two outputs</span>
<span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">():</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">jp</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">jp</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">c</span> <span class="o">=</span> <span class="n">jp</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jax_multiarg</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>

<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="dlpack">
<span id="id1"></span><h2>DLPack<a class="headerlink" href="#dlpack" title="Link to this heading">#</a></h2>
<p>Warp supports the DLPack protocol included in the Python Array API standard v2022.12.
See the <a class="reference external" href="https://dmlc.github.io/dlpack/latest/python_spec.html">Python Specification for DLPack</a> for reference.</p>
<p>The canonical way to import an external array into Warp is using the <code class="docutils literal notranslate"><span class="pre">warp.from_dlpack()</span></code> function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">warp_array</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">external_array</span><span class="p">)</span>
</pre></div>
</div>
<p>The external array can be a PyTorch tensor, Jax array, or any other array type compatible with this version of the DLPack protocol.
For CUDA arrays, this approach requires the producer to perform stream synchronization which ensures that operations on the array
are ordered correctly.  The <code class="docutils literal notranslate"><span class="pre">warp.from_dlpack()</span></code> function asks the producer to synchronize the current Warp stream on the device where
the array resides.  Thus it should be safe to use the array in Warp kernels on that device without any additional synchronization.</p>
<p>The canonical way to export a Warp array to an external framework is to use the <code class="docutils literal notranslate"><span class="pre">from_dlpack()</span></code> function in that framework:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">jax_array</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">warp_array</span><span class="p">)</span>
<span class="n">torch_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">warp_array</span><span class="p">)</span>
</pre></div>
</div>
<p>For CUDA arrays, this will synchronize the current stream of the consumer framework with the current Warp stream on the array’s device.
Thus it should be safe to use the wrapped array in the consumer framework, even if the array was previously used in a Warp kernel
on the device.</p>
<p>Alternatively, arrays can be shared by explicitly creating PyCapsules using a <code class="docutils literal notranslate"><span class="pre">to_dlpack()</span></code> function provided by the producer framework.
This approach may be used for older versions of frameworks that do not support the v2022.12 standard:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">warp_array1</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">(</span><span class="n">jax_array</span><span class="p">))</span>
<span class="n">warp_array2</span> <span class="o">=</span> <span class="n">wp</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">(</span><span class="n">torch_tensor</span><span class="p">))</span>

<span class="n">jax_array</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">(</span><span class="n">warp_array</span><span class="p">))</span>
<span class="n">torch_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">dlpack</span><span class="o">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">wp</span><span class="o">.</span><span class="n">to_dlpack</span><span class="p">(</span><span class="n">warp_array</span><span class="p">))</span>
</pre></div>
</div>
<p>This approach is generally faster because it skips any stream synchronization, but another solution must be used to ensure correct
ordering of operations.  In situations where no synchronization is required, using this approach can yield better performance.
This may be a good choice in situations like these:</p>
<blockquote>
<div><ul class="simple">
<li><p>The external framework is using the synchronous CUDA default stream.</p></li>
<li><p>Warp and the external framework are using the same CUDA stream.</p></li>
<li><p>Another synchronization mechanism is already in place.</p></li>
</ul>
</div></blockquote>
<dl class="py function">
<dt class="sig sig-object py" id="warp.from_dlpack">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">from_dlpack</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">source</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#warp.from_dlpack" title="Link to this definition">#</a></dt>
<dd><p>Convert a source array or DLPack capsule into a Warp array without copying.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>source</strong> – A DLPack-compatible array or PyCapsule</p></li>
<li><p><strong>dtype</strong> – An optional Warp data type to interpret the source data.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A new Warp array that uses the same underlying memory as the input
pycapsule.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="runtime.html#warp.array" title="warp.types.array"><em>array</em></a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="warp.to_dlpack">
<span class="sig-prename descclassname"><span class="pre">warp.</span></span><span class="sig-name descname"><span class="pre">to_dlpack</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">wp_array</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#warp.to_dlpack" title="Link to this definition">#</a></dt>
<dd><p>Convert a Warp array to another type of dlpack compatible array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>wp_array</strong> (<a class="reference internal" href="runtime.html#warp.array" title="warp.types.array"><em>array</em></a>) – The source Warp array that will be converted.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A capsule containing a DLManagedTensor that can be converted
to another array type without copying the underlying memory.</p>
</dd>
</dl>
</dd></dl>

</section>
</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          <a class="next-page" href="../configuration.html">
              <div class="page-info">
                <div class="context">
                  <span>Next</span>
                </div>
                <div class="title">Runtime Settings</div>
              </div>
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
            </a>
          <a class="prev-page" href="generics.html">
              <svg class="furo-related-icon"><use href="#svg-arrow-right"></use></svg>
              <div class="page-info">
                <div class="context">
                  <span>Previous</span>
                </div>
                
                <div class="title">Generics</div>
                
              </div>
            </a>
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2022-2024, NVIDIA
            </div>
            Made with 
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link " href="https://github.com/NVIDIA/warp" aria-label="GitHub">
            <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16">
                <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path>
            </svg>
        </a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">Interoperability</a><ul>
<li><a class="reference internal" href="#numpy">NumPy</a><ul>
<li><a class="reference internal" href="#warp.dtype_from_numpy"><code class="docutils literal notranslate"><span class="pre">dtype_from_numpy()</span></code></a></li>
<li><a class="reference internal" href="#warp.dtype_to_numpy"><code class="docutils literal notranslate"><span class="pre">dtype_to_numpy()</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#pytorch">PyTorch</a><ul>
<li><a class="reference internal" href="#example-optimization-using-warp-from-torch">Example: Optimization using <code class="docutils literal notranslate"><span class="pre">warp.from_torch()</span></code></a></li>
<li><a class="reference internal" href="#example-optimization-using-warp-to-torch">Example: Optimization using <code class="docutils literal notranslate"><span class="pre">warp.to_torch</span></code></a></li>
<li><a class="reference internal" href="#example-optimization-using-torch-autograd-function">Example: Optimization using <code class="docutils literal notranslate"><span class="pre">torch.autograd.function</span></code></a><ul>
<li><a class="reference internal" href="#warp.from_torch"><code class="docutils literal notranslate"><span class="pre">from_torch()</span></code></a></li>
<li><a class="reference internal" href="#warp.to_torch"><code class="docutils literal notranslate"><span class="pre">to_torch()</span></code></a></li>
<li><a class="reference internal" href="#warp.device_from_torch"><code class="docutils literal notranslate"><span class="pre">device_from_torch()</span></code></a></li>
<li><a class="reference internal" href="#warp.device_to_torch"><code class="docutils literal notranslate"><span class="pre">device_to_torch()</span></code></a></li>
<li><a class="reference internal" href="#warp.dtype_from_torch"><code class="docutils literal notranslate"><span class="pre">dtype_from_torch()</span></code></a></li>
<li><a class="reference internal" href="#warp.dtype_to_torch"><code class="docutils literal notranslate"><span class="pre">dtype_to_torch()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#cupy-numba">CuPy/Numba</a></li>
<li><a class="reference internal" href="#jax">JAX</a><ul>
<li><a class="reference internal" href="#warp.from_jax"><code class="docutils literal notranslate"><span class="pre">from_jax()</span></code></a></li>
<li><a class="reference internal" href="#warp.to_jax"><code class="docutils literal notranslate"><span class="pre">to_jax()</span></code></a></li>
<li><a class="reference internal" href="#warp.device_from_jax"><code class="docutils literal notranslate"><span class="pre">device_from_jax()</span></code></a></li>
<li><a class="reference internal" href="#warp.device_to_jax"><code class="docutils literal notranslate"><span class="pre">device_to_jax()</span></code></a></li>
<li><a class="reference internal" href="#warp.dtype_from_jax"><code class="docutils literal notranslate"><span class="pre">dtype_from_jax()</span></code></a></li>
<li><a class="reference internal" href="#warp.dtype_to_jax"><code class="docutils literal notranslate"><span class="pre">dtype_to_jax()</span></code></a></li>
<li><a class="reference internal" href="#using-warp-kernels-as-jax-primitives">Using Warp kernels as JAX primitives</a></li>
</ul>
</li>
<li><a class="reference internal" href="#dlpack">DLPack</a><ul>
<li><a class="reference internal" href="#warp.from_dlpack"><code class="docutils literal notranslate"><span class="pre">from_dlpack()</span></code></a></li>
<li><a class="reference internal" href="#warp.to_dlpack"><code class="docutils literal notranslate"><span class="pre">to_dlpack()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=1ed6394b"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=32e29ea5"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=fd10adb8"></script>
    </body>
</html>